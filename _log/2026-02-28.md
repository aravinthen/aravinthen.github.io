---
layout: log
title: Reading papers in the airport
date: 2026-02-28
math: True
tags: [reinforcement-learning]
---

## Summary
* Finished reading *Deterministic Policy Gradient Algorithms* by Silver et al.
* Got stuck on a backtracking problem and then learned how to solve it properly.
* Spent some time working on my classical reinforcement learning algorithm study.

## Notes
### Literature
1. I wasn't sufficiently impressed by the main power of the policy gradient theorem, that the gradient itself doesn't actually depend on the state distribution - that's a major strength here. I expect this will be very handy when I try and implement a policy gradient algorithm from scratch in a few weeks time.
2. The conditions provided in the paper for removing bias in the actor-critic architecture are  suprising - the first is that the critic has to follow a particular functional form (linearity) whereas the second is that the parameters have minimise the mean square error. So... essentially, you have to have to represent the critic as a linear regressor in order to ensure no bias. How does this consideration of bias extend to deep networks? I daresay that I'll get an answer to this sooner rather than later.
3. A new framing of off-policy: the training objective becomes *the expected return of the  action-value function*.
4. Ah, the main observation that powers this new technique is the realisation that gradients in the Q-value provide a useful direction in which to update a policy.
5. The crux of this technique is to decompose the policy gradient into the gradient of Q with respect to the actions as well as the gradient of the policy with respect to the policy parameters. That allows the basic localized improvement rule from the first point to apply for the deterministic case: as such, the *deterministic* policy gradient theorem is a version that requires the gradient of the policy with respect to its parameters *and* the action-value function with respect to the actions. 
6. Ah, I get it! Basically,
    * The stochastic policy gradient weights the action gradient by the log-probability of the action, whilst
    * the deterministic policy gradient directly follows the gradient of the Q-function to the better action.
7. It's really important to consider regimes where your policy changes too quickly. The rates of change here are *extremely* important, with such rapid shifts becoming very prominent in high dimensions. 

### Algorithms
* I was trying to solve the `Generate Parenthesis` problem today. I screwed it up: built a solution that generated *all* parenthesis pairs and as a result blew up in memory. The actual solution to the puzzle is nice and requires two extra states: managing the way that those two states interacted with the string that they were generating was the key to the problem.

### RL study
* More progress on setting up the experiments modules. I've switched from coding in `Emacs` to writing my stuff in `PyCharm` - I daresay I'll be moving to `Cursor` before long as well. I think it's time to stop being so stubborn on the tools that I use: now is time to get results, not get too invested in tooling.

## Remarks  
1. Spent a lot of time at Heathrow today. I'm not flying anywhere (for now), but my girlfriend is heading to India and I'm hanging around here until she's on the flight in case the major international conflict that just started causes issues in her travel.