---
layout: log
title: Definitely convolutional
date: 2026-01-22
math: True
tags: [deep-learning, reinforcement-learning]
---

## Summary
* Started reading *Convolutional Sequence-to-Sequence Learning* by Gehring et. al. 
* More progress on Sutton and Barto - finished Chapter 4.

## Notes
1. Convolve a network enough and you'll reach a point where every part of the input in some way influences the output. You could balance local attention and global attention by simply controlling the depth of the convolutional neural network that you apply for your attention mechanism.

### Sutton and Barto 
1. ~I must confess that I skipped the programming exercise 4.7. It's a pretty significant block of code! I'm content to get the thematic understanding behind dynamic programming, but the fact of the matter is that I probably won't need to implement this myself given that my goal is *deep* reinforcement learning.~ Am I being a dumbass here? Dynamic programming can handle millions of states... it could be a useful way of building benchline agents based on how you discretise the problem. Okay, I will revisit this exercise soon after I've investigated the possibility.
2. Question 4.8 is really, really weird. I realise that the value is based purely on *reaching* the goal state versus the intermediary points: I'm guessing that 50 is a special state because it directly maximises value by winning in the next step, but I'm not sure if this is the full solution for the problem. 
3. The chances of winning the game at V(50) is the probability of heads, $$p$$. This is the maximum chance of winning for all states in the environment. I guess by value iteration, we're just going ahead and maximising the probability of winning by state...

## Remarks
1. ...called it yesterday. :) I'm happy about this - it means that this deep immersion into the field of deep learning is actively changing the way I look at things.  
2. On the downside, I wasn't able to read much today. Had to work through my lunch break. :(  
3. I have a mental map of how the encoder-decoder archicture works now. I think the key point here - the *essential* point - is the way the state is built up. The encoder *builds* the state - it is responsible for encoding the variable size input sentence in a way that compresses its full context. The decoder then takes that compressed representation and passes it continuously through another RNN to provide a prediction... and finally, the decoder will output <end-of-line>. This week's literature review is going to be good. :) 