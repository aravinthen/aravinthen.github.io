---
layout: log
title: The whole wide world
date: 2026-01-28
math: True
tags: [reinforcement-learning]
---

## Summary
* Read *World Models* by David and Schmidhuber.
* More progress on Sutton and Barto - finished Chapter 5.
 
## Notes
1. An essential motivation for using a world model (which is essentially just a compressed model of the environment) is to remove the need for an agent to interact with the environment itself. This is purely a question of training efficiency: smaller networks are easier to train. You can make a model smaller by essentially *decoupling* the part of it that has to navigate world modelling.
2. You essentially break your agent into a *world-model* and a *controller*. That's smart! It also means that you can build in predictive capabilities into your agent by using the world model to predict future states.
3. A *variational autoencoder* is used to compress an image into a probabilistic latent vector. I haven't yet read about these models: it should be added to my literature review list. :) 
4. It's worth mentioning that the agent never really *sees* the state of the environment. All it ever sees is a latent representation.
5. In the VizDoom section, the authors show how they augment the prediction mechanism to predict death. They also *don't need to obtain rewards*, which means they can train entirely on the memory model (I think?). ~~I need to sit down and think about this~~ Ah yes, of course: the memory model predicts the *future* step, so you can use it for system identification. This is quite similar to the graph networks as physics engines approach that I studied on 05/01/2026.
6. This paper is really, really elegant. There's an aesthetic quality to it. The results are also really impressive!

### Sutton and Barto
* *"In on-policy methods, the agent also explores, but learns a deterministic optimal policy that may be unrelated to the policy followed".* This is absolutely key here. 
    * You use on-policy methods if reacting with some degree of randomness based on the scenario is beneficial.
    * You use *off-policy* methods if you want a deterministic policy that you've honed using a non-deterministic policy. 
* Also, bootstrapping: using prior value estimates to estimate current value. This is a theme that is popping up repeatedly over the course of reading.
* Now we're on Chapter 6. This one is going to be important...

## Remarks
* My parents used to take a particular route to Watford whenever we went to visit family there. One part of the route was a particularly scenic viewpoint of the entire town from higher ground. As a child, I used to believe that I was seeing "the whole wide world" - I now realise that my memory network was just poorly calibrated. :)
* Steady progress on Sutton and Barto. I am actually solving most of the exercises! I'll be honest, I'm skipping a few of the more intricate problems that usually involve coding. Such is the compromise!