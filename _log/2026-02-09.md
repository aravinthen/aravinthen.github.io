---
layout: log
title: Hunger 
date: 2026-02-09
math: True
tags: [graph-neural-networks, reinforcement-learning]
---

## Summary
* Read *How powerful are graph neural networks?* by Xu et al. 
* Spent some time studying binary search trees. 

## Notes
1. Representation learning is the term I keep forgetting to describe what graph neural networks do. Just as a recap, representation learning is a field of ML that allows systems to automatically discover, extract, and learn relevant features from raw data. 
2. This is a very, very interesting paper. It is the first time I've come across a piece of work that actually measures the level of expressivity of a network and ties it to non-ML based methods.
3. It's interesting to see how the language of graph neural networks evolved in the time. Readout still exists and refers to the same thing as per Gilmer et al., but message passing and update have been replaced with "aggregate" and "combine"!
4. Weisfeiler-Lehman test - a means of distinguishing between classes of graphs. From my understanding, it's a means of augmenting feature representations with associated labels and the carrying out the basic message passing formalisms seen in GNNS, after which a the algorithm decides if two graphs are different if any of those labels are differ.
5. Expressive power of a graph neural network is defined as the way the network maps similar components. A maximally expressive GNN will *only* ever map two nodes to the same embedding if the features and topology underlying them are utterly identical.
6. ARRGHH. SET THEORY. GET IT AWAY FROM ME I'M A PHYSICIST I DON'T NEED THIS RIGOUR IN MY LIFE.
7. There is something interesting here in that equivalence between the Weisfeiler-Lehman test and the GNN is defined in terms of differences in the embedding... it seems as though the WL test is a theoretical limit on what GNNs can represent.
8. The power here is the GNNs represent a more general way of representing the Weisfeiler-Lehman test: they *learn* embeddings, whereas the WL test relies on one-hot encoding. In a sense, a GNN could potentially be thought of as a function approximator for the WL test!
9. ...and that is precisely what a graph isomorphism network is.
10. This is a pretty cool way of looking at expressivity in general. I'm going to have to give this paper a second pass when I do my literature review, but I think I've cracked the essence of it.

## Remarks
* Forgot to eat lunch today. Not good. Reading this paper felt torturous, but there's a weird level of focus that emerges when you're starving. 
* Ran out of time to make progress on Sutton and Barto. A number of factors contributed to this, but I need to do better here. I would ideally like to have this book finished within the month.