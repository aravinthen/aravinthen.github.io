---
layout: log
title: Transformers in the silicon
date: 2026-02-07
math: True
tags: [deep-learning, reinforcement-learning]
---

## Summary
* Wrote up literature review for the week.
* More progress on Sutton and Barto - Finished Chapter 6.
* Spent a bit of time trying to figure out transformer masks. 


## Notes
1. Good literature review today. A lot of themes came into their own and I realised that I just had to look at the full picture to tie things together. I'm confident that at the very least I can talk about graph neural networks intelligently. This is a damn sight better than I was when I started reading about them a month ago! As it happens, I really needed was to sit down and go through the foundational papers.
2. I was trying to figure out the masks today. I think I've gotten it, but I need to check my implementation - this will sadly have to wait until I can work on deep architectures again. I'm afraid that this is not the result I wanted for this weekend, but one must adapt. 
 
### Sutton and Barto
1. I spent a lot more time that I'd like to admit looking at double Q-learning to try and figure out what is actually going on here. I decided to just sit down and derive the whole thing more explicitly. 

Let's start with the maximisation bias. It's just what happens you have noisy systems - in general, given the Q-function update as
$$
Q(s,a) \leftarrow Q(s,a) + \alpha\left(r + \gamma \max_{a'} Q(s',a') - Q(s,a)\right),
$$,
if the true Q-value is given by 
$$
Q(s', a') = Q^\*(s', a') + \varepsilon_{a'},
$$
then the expected value of $$\max_{a'} Q(s',a')$$ will be larger than $$\max_{a'} Q^\*(s',a')$$ itself. It took me a bit of time to realise, but it just so happens that this is something that can poison the learning process as a whole: every update to $Q$ will implicitly bleed into other state/action estimates due to bootstrapping.

The issue here is that the *same* estimator is used twice:
* in selecting the action, which is what the target policy does, and
* in evaluating the action's value.

Double Q-learning is based around using *two* Q-functions, each to satisfy these two tasks. You're breaking up the learning process and isolating the effects of noise, something that roughly speaking cancels out the maximisation bias.

Say you have two action-value estimators $$Q_A$$ and $$Q_B$$. You use $$Q_A$$ to inform your greedy policy and $$Q_B$$ to evaluate the value of the actions. The update rule becomes
$$
Q_A(s,a) \leftarrow Q_A(s,a) + \alpha\left(r + \gamma Q_B(s', \max_{a'} Q_A(s,a)) - Q_A(s,a)\right),
$$
and symmetrically for $$Q_B$$.

It's a really, *really* elegant technique! I've heard of DDQN learning - looking forward to seeing how this technique finds use in the function approximator chapters.

## Remarks
1. I need to speed it up when it comes to deep reinforcement learning. I have to remember that my fundamental goal here is to build these systems: not simply understand the theory!
2. My architecture project is going poorly. I'm learning that deep learning architectures aren't projects that you can simply code up over a weekend from the get go: this is a skill that I'm going to have to work on before I can do it effortlessly.