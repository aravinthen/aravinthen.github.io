---
layout: log
title: Paying too much attention 
date: 2026-01-21
math: True
tags: [deep-learning, reinforcement-learning]
---

## Summary
* Finished reading *Effective approaches to attention-based neural machine translation* by Luong et al.  
* More work on Sutton and Barto.

## Notes
1. The basic difference between Bahdenau and Luong is the introduction of the window of localized attention. I note that the window size is empirically selected: this was the main mental block for me when reading the paper, as I was implicitly assuming that it was a parameter.
2. Adding detail to a passage from the paper,
    * The model first predicts a single aligned position for the current target word, where alignment is the process of establishing correspondences between words in the source sentence and the target sentence.
    * A window centered around the source position is then used to compute a context vector, which is a a weighted average of the source hidden states in the window.
    *  The weights at are inferred from the current target state and the source states included within the window.
3. The distinction between monotonic alignment and predictive alignment is interesting. The aim of both is to find a "center point" within the sentence, but monotonic alignment represents a one-to-one correspondence between the source sentence and the output sentence whereas predictive alignment attempts to infer which source input is most appropriate. 

### Sutton and Barto
I normally scrawl my solutions on a notepad, but I'm going to include my solution for Exercise 4.5 here given how important it is to the rest of the textbook. It'll also provide a testbed for the `mathjax` that I just set up. :)

**Question**: How would policy iteration be defined for action values? Give a complete
algorithm for computing $$q_*$$, analogous to that on page 80 for computing $$v_*$$. Please pay special attention to this exercise, because the ideas involved will be used throughout the
rest of the book.

**Solution**:
Let's recall the policy algorithm described in the previous log. First initialise $$V(s) \in \mathbb{R}$$ and $$\pi(s) \in \mathcal{A}(s)$$ arbitrarily for all $$s \in \mathcal{S}$$.

We then carry out policy evaluation, where 

$$
\hspace{0pt}
\begin{array}{l}
\text{Loop:} \\
\qquad \Delta \leftarrow 0 \\
\qquad \text{for each } s \in \mathcal{S} \text{ do} \\
\qquad\qquad v \leftarrow V(s) \\
\qquad\qquad V(s) \leftarrow \sum_{s',r} p(s', r \mid s, \pi(s))
  \left[ r + \gamma V(s') \right] \\
\qquad\qquad \Delta \leftarrow \max(\Delta, |v - V(s)|) \\
\qquad \text{until } \Delta < \theta
\end{array}
$$


## Remarks  
1. There is a weak correspondence between local attention and convolution kernels - I'm saying weak because I haven't yet looked too deeply into this. 
    * **Edit**: as it happens, the next paper that I need to read is called "Convolutional sequence to sequence learning". Nice. :)