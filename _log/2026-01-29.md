---
layout: log
title: Too complexodynamic for me
date: 2026-01-29
math: True
tags: [machine-learning, reinforcement-learning]
---

## Summary
* Read *The First Law of Complexodynamics* by Scott Aaronson.
* More progress on Sutton and Barto.
 
## Notes
1. I've been following Aaronson for a while, mostly because of my side interest in quantum computing. He's a very engaging writer, which is something that can be seen on full display in this particular blog post.
2. The main question the post tries to answer is this: "Why does “complexity” or “interestingness” of physical systems seem to increase with time and then hit a maximum and decrease, in contrast to the entropy, which of course increases monotonically?"
3. The distinction between complexity and entropy is an important one and something that I missed when I was looking into complex systems way back when. There may be some answers to this question in biophysics: I vaguely remember reading a thermodynamic argument for the wild complexity of life stemming from the dissipation of excess free energy that is provided by the Sun to the Earth.
4. Anyway, enough navel-gazing. What on Earth does this have to do with AI? A lot. Where to start?
    * Take the process of training a neural network. Initially, you have highly randomized weights that are completely generic. This is a low-complextropy regime. 
    * You keep training and the network begins to develop structure. This is high complextropy, where genuinely complex patterns emerge in what was once a random structure. 
    * If you overfit, the weights are highly specified based on the training data. They really may as well be completely random.
5. Let's think about a neural network as a computer program. Sutskever et al. describe this briefly in *Sequence-to-sequence learning with neural networks*, where a deep neural network is able to learn how to sort numbers. Taking this task as an example, 
    * Say you start off with a small number of weights. This is a weaker model that will likely give you random results, just because it can't actually memorise any useful program.
    * A larger network will probably be able to learn a program that that can sort the numbers. Awesome - we're in a high-complextropy domain.
    * A massive, enormous, highly complex neural network will require a *lot* of training to accomplish the same task. It will likely tend to overfit, following the same low complextropy described in the last point in the previous.
6. I get why Sutskever recommended this paper. It is a very powerful way of looking at neural networks.

### Sutton and Barto
1. Temporal difference methods add a timestepping process to the original MC technique. It's kind of like a hybrid between Monte Carlo and molecular dynamics: cool, since the same hybrid is used in computational statistical physics to great effect.
2. I know that this stuff is going to blow my mind. Just on a mechanical level, the way the incremental evaluation method show in Chapters 2 and 5.6 are being merged and employed is really nice.

## Remarks
* I had a 10 hour work day today doing some really, *really* interesting stuff. Shame I can't talk about it! 
*  ~~Reading *The First Law of Complexodynamics* was admittedly a bit of a cop-out. But hey, it's on Sutskever's list!~~ This was no cop-out. It really made me think. :) 