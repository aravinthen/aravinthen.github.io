---
layout: log
title: Travel log
date: 2026-01-18
tags: [software-engineering, reinforcement-learning]
---

## Summary
* More progress on *Effective Python* on the train back to Bristol - finished Chapter 3.
* More progress on Sutton and Barto - finished chapter 3. :) 

## Notes
1. I forgot to bring my Kindle on my weekend trip to London, which meant that I was "forced" to read software engineering the whole way through. This wasn't actually so bad - it was quite easy reading, not in the least because Brett Slatkin is a remarkably lucid writer.  
2. It's also remarkable how many of the tools and ideas that I've just picked up over the years of using Python as my main language for science and engineering. It's really nice to fill in these gaps.
3. Now, there are a range of tools I *haven't* used extensively during my Python career. One of these is the *nonlocal* statement - I've been in a ton of situations where I've written functions with extra command logic to handle what could have been done with `nonlocal`. Granted, this definitely isn't a technique to get into habit of - I've fallen for too many global variables traps to trust `nonlocal` too much. :)
4. Studying reinforcement learning reinforces the idea that neural networks (and intelligence in general) is nothing more than the problem of compression in extremely high dimensional spaces. That is *literally* all this is: compression. 

## Remarks
1. I admit that my physicist instincts have failed me over the course of reading Sutton and Barto: I wasn't actually aware of the connection between the Bellman equations and Hamilton's principal function. Absolutely *beautiful* thing to realise and something that is philosophically very interesting to me. 