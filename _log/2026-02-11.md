---
layout: log
title: Forgetfulness 
date: 2026-02-11
math: True
tags: [reinforcement-learning, distributed-computing]
---

## Summary
* Read *RLlib: Abstractions for Distributed Reinforcement Learning* by Liang et al.

## Notes
1. The paper notes that RL system design is lacking. This is something that I've noticed too - there are advanced methods by which to conduct reinforcement learning (Anakin and Selbulba architectures come to mind), but in general there doesn't seem to be any standardized means by which RL can be carried out.
2. The `RLlib` contributors seek to fix this via the introduction of composable parallel primitives. They use the `Ray` library, specifically the Actor structure (which is just a class that implements functionality). 
3. I think the real takeaway from this is that it is difficult to reuse distributed components for different algorithms. I didn't realise that this was something that we had to be aware of - that different *algorithms* required vastly different system architectures... 
4. RLlib aims to fix these issues by the power of modularity. `TorchRL` attempts to do the same.
5. The paper identifies multiple levels of control, each focused on the marshalling of workers against a driver resource. The distinction that these guys introduce is the emergence of hierarchical delegated control from logically centralized control. The key insight of the work is that all RL algorithms can in some way be written in the framework of logically centralized control.
6. There are a range of important abstractions here that are used to organise the `RLlib` process. For example, the software employs 
    * parameter servers, which are distributed computing architecture designed to scale machine learning model training across multiple machines by separating training into worker nodes (computing gradients) and server nodes (managing model parameters),
    * collective communication, a parallel programming model where all processes in a defined group (communicator) participate in a single, coordinated operation. It seems like this is based around MPI style primitives, which means that the collective communication framework  will typically focus on synchronization, data movement and global computation. Part of this (that is mentioned in the paper) is the `all-reduce` operation.
7. It's interesting to note that RL lib specifies the policy and its transitions direclty. A policy evaluator class is provided and this also implies a separation between actor and learner. 
8. My main takeaway from this paper is that I need to start learning `Ray`, which is a task-based programming paradigm. It seems to be far too useful in distributed computing to just leave as a skill to pick up over my career. In fact, I think that this might be a good opportunity to start incorporating a formal software engineering plan into my routine outside of just randomly reading from *Effective Python* every now and then.

## Remarks
* I'm currently travelling for a work event. I was in such a rush to get to the train station that I forgot to pack my iPad, which is where I do most of my reading activity. Oops!
* This is fine! I can still read stuff. However, given that I can't type, its unlikely that my log for today is going to be very populated.
* In fact, I'm going to write my notes on paper and the transcribe them tomorrow once I'm back from this work trip.
