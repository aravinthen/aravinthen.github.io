---
layout: log
title: Transformers: graph networks in disguise
date: 2026-02-19
math: True
tags: [deep-learning, backtracking, ]
---

## Summary
* Read *A Generalization of Transformer Networks to Graphs* by Dwivedi and Bresson.
* Reminded myself what backtracking means.

## Notes
### Literature 
1. Starting strong. In a sense, the representation of a transformer as being a neural network over fully connected graphs is really new.
2. The advances of this paper:
    * Attention being a function of neighbourhood connectivity. This makes sense: it's quite similar to the graph attention mechanism described in the GAT message passing layer from a few weeks ago.
    * Positional encodings via Laplacian eigenvectors. Oof. Going to have to dig into the mathematical detail here...
    * Layer normalization in the transformer being replaced by batch normalization. Okay, makes sense: normalize the input between layers, almost like in-built regularization.
    * Edge feature representation. This pushes the architecture closer to the GN framework in terms of expressibility. 
3. This paper is to Velickovic et al. what Vaswani et al. was to Bahdenau et al. The attention mechanism described in this paper is *way* closer to the full transformer architecture.
4. One takeaway that I'm getting from this paper is that in situations where graphing is *difficult*, you may as well just use a full transformer... because transformers are effectively graph neural networks on a fully-connected graph. In a sense, **self-attention is message passing between tokens**. It's almost as though you're letting the transformer figure out its edges via training.
5. I'm not going to lie, I'm a little lost at the use of laplacian eigenvectors as a means of representing positional encodings, especially in analogy to sinuso√Ødal positional encodings. I need to think about this a little more in context with the latter.
6. The essence here is that graph transformers reduce the need to train a transformer by including inductive bias - in this case, the topology of the graph being modelled. I will definitely need to read some supplementary material to crack the depth of this connection, though.

### Algorithms
1. The topic today was backtracking, which according to a few online tutorials is explicitly a technique used to handle combinatorial search problems.
2. As it happens, combinatorial search problems can effectively be handled as a depth-first search on a state space tree. Backtracking is a means of controlling the size of a state space tree: you're converting a wild and exponential state space tree into a nicely pruned and manageable traversal.
3. An important part of backtracking comes from *construction*. We are never given the tree by itself! We construct the relevant parts of the tree as we proceed. 

## Remarks 
* Used my lunchbreak to set up a collection with a former colleague and some partners at a quantum computing company today. As a result, I didn't do my literature review during lunch time, so I did it when I got home. This meant less time for the RL project! Such is life. 