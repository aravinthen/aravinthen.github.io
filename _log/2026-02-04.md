---
layout: log
title: Starting to pay attention (to graphs)
date: 2026-02-04
math: True
tags: [graph-neural-networks, reinforcement-learning]
---

## Summary
* Read *Graph Attention Networks* by Velickovic et al.
* More work on Sutton and Barto.

## Notes
* The paper distinguishes between graph representations as spectral and non-spectral. Having looked into it, it seems that the spectral representation of a graph is the multiset of eigenvalues and eigenvectors that derived from a matrix that represents the graph's connectivity (the Laplacian matrix). 
* The basic essence of what is going on here is that you can *motivate* convolutions on a graph by following the approach of representing the graph as a spectral representation and taking a Fourier transform of filter that represents the convolution. This is, in my head, the fancy-schmancy mathematical approach to carrying out convolutions.
* The approach we saw in the *previous* paper was non-spectral, but nevertheless approximated the spectral approach by appealing to the representation of the filters as Chebyshev polynomial expansions.
* "The idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention strategy". There we go... where you have convolutions, you can also have attention. ~~It's almost as though attention is essentially a convolution over time (or order)...~~ I can't say this, this is not rigorous enough to internalize. 
* It's interesting to see that the authors of this paper basically require a linear layer in between the node representations and the graph. This is to provide sufficient expressive power.
* The basic steps of the paper:
    1. Expand the node features via a linear layer.
    2. Define a shared attention mechanism between initial node and the nodes within its neighbourhood. The paper employs an attention mechanism that concatenates two feature representations, passes them through another linear layer and then passes the result through a leaky rectified linear unit. 
    3. The attention between all neighbourhood pairs using softmax. The node representations are then weighted with their respective attentions and summed, after which a nonlinearity is optionally applied. 
    4. Multiheaded attention is also used here. This is conducted via concatenation.
* Results are either superior to or about as good as `GraphSAGE`, which is to be expected. What I'm getting here is that `GraphSAGE` is something of a inductive and localised version of convolution over graphs, whereas `GAT` is the equivalent for attention mechanisms. 
* This leads to the question: what is the foundational difference between between `GraphSAGE` and GCNs? I'm guessing that GCNs are transductive in that they require the whole graph, but I should probably check the original papers. 

### Sutton and Barto
1. The difference between the Monte Carlo and Temporal Difference methods is that 
    * MC waits until the end of an episode to update the value function estimate, whereas TD updates immediately after each step. MC *demands* that you calculate the true return before providing estimates. 
    * TD uses bootstrapping (when other estimates are used to improve the value of the current estimate). MC uses the true rewards to build estimates of the value function. 
2. I suppose that this means that TD is better suited to continuous tasks than MC. You are literally just learning from your current predictions - your running tally - instead of waiting until you find the actual value of that which you're trying to predict...
3. I can intuitively understand that there's a trade-off in using TD versus MC. In fact, it's quite literally a bias-variance trade-off. TD has lower variance because it's estimates are based on the next step rather than the full episode. However, this choice adds a large amount of bias!
4. Useful tool here: Markov reward processes. I can think of a few occasions at where this might be useful...
5. This chapter demonstrates the use of *batch updating*, where you calculate a running sum on increments but update the value function only once. This is important terminology.
6. As it happens, the TD(0) method *fully* approximates the *Markov chain*, whereas the MC model doesn't exploit the memoryless structure of a Markov chain. This is the heart of the *certainty-equivalence estimate*: if it just so happens that the TD method fully approximates the actual Markov chain, you will absolutely get the correct expected value. 
7. 

## Remarks
* Slow day today - was super tired due to being sleep deprived. Took a 10 minute nap and managed to get some work done. :) 
* I realised today that despite my role as an algorithm engineer, I don't really consider myself to have mastered algorithms as most software engineers drill them. I mean sure, I can look up depth-first search and implement it, but I don't think I'd be able to blast out a complex data structures and algorithms puzzle from scratch.
* My colleague mentioned a story about how the guy who wrote `brew` wasn't hired by Google because he couldn't solve a dynamic programming puzzle. I daresay the moral of that story is that you don't have to be an algorithms expert to build popular, useful software. However, I'm not a software engineer. I'm an algorithm engineer. I should know my algorithms. 