---
layout: log
title: Convolutional or convoluted?
date: 2026-01-23
math: True
tags: [deep-learning, reinforcement-learning]
---

## Summary
* Finished reading *Convolutional Sequence-to-Sequence Learning* by Gehring et. al.  
* Read the abstract of *Attention is All You Need* by Vaswani et al. 
* Made progress on Sutton and Barto.

## Notes
### *Convolutional Sequence-to-Sequence Learning* by Gehring et. al.  
* This is a complex paper. It's definitely harder than the stuff I've read so far. However, for some reason I was *really* struggling to parse it yesterday whereas I'm cutting through it alright now. 
* The essence of the convolutional approach is a means to defeat the issues that RNN based architectures cause - sequence dependencies and computational inefficiency.  
* If you strip away the recurrent network, the positional encoding used in the convolutional sequence-to-sequence model is a necessary. Recurrent networks don't need positional encodings!
* The essence of this paper is still the same as what I realised yesterday: stack enough convolutions and you'll be able to defeat the problem of sequence dependency.
* The fact that each decoder layer has a relevant attention module is cool - you can see how this works quite clearly in the way that the architecture is represented in the graphic. This would be useful in learning multiple contextual focuses too! It's kind of like how convolutional networks have layers that progressively start to capture essential features at a coarser level of detail: a bunch of pixels become a line, a few lines become a shape, a few shapes become an eye... so on.
* I guess the point here is that you *don't* need to have a sequential architecture in order to learn context. That's a bit of a revelation once you think about it...

## Attention is All You Need by Vaswani et al.  
* Just from the abstract I know this paper is going to shock me. **Just from the abstract!** Tomorrow is going to be an interesting day. :) 

## Sutton and Barto
* Back to Monte Carlo methods. I have a long history with Monte Carlo methods - I used to develop them for applications in polymer physics. :) 
* As anticipated, this method is quite simple on a conceptual level. You kinda just
    1. Initialise a value function, 
    2. Sample a state for that specific policy and then switch to it.
    3. Generate an episode rollout from that state,
    4. For every state visited in the rollout,
        1. Iteratively calculate the expected return,
        2. Append it to the list of returns for that state,
        3. Assign the average of the list of returns to the value of that state.
    5. Repeat from Step 2.
* Policy improvement is also referred to in the book as *control*.
* "In GPI one maintains both an approximate policy and evaluation an approximate value function. The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function." - this point has been repeated at least four times now. It is a fundamental way of thinking about the reinforcement learning problem.
* Ah, here's a formal definition for some terms that I've been coming across a lot:
    * **On-policy method**: a method that attempts to evaluate or improve the policy that is used to make decisions.
    * **Off-policy method**: a method used to evaluate or improve a policy different from the one used to generate the data. 
* There is a huge level of depth that emerges from using an off-policy learning. There's another piece of terminology relevant here: the *prediction problem*: the goal of finding the optimum policy using data from a policy that is not the optimum. 
* There we go. Importance sampling - a mainstay of computational statistical mechanics.
 
## Remarks
1. The paper today was super interesting. It's *deep* - you can just tell that the idea of convolutional neural networks emerged from a sudden jolt of genius.
2. Good old Monte Carlo methods. I love them! I spent so much time as a statistical physicist thinking about problems in terms of ensembles of states. I don't think this is typical of most physicists, who usually think about physics as a process of dynamical evolution. Of course, molecular dynamics is very much a means of generating states by dynamical means - perhaps the mindset here is to think of dynamic programming as MD and MC as... uh... MC?