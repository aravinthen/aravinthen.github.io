---
layout: log
title: Messy sequences
date: 2026-01-13
tags: [deep-learning]
---

## Summary
* Finished reading *Sequence-to-sequence learning with Neural Networks* by Sutskever, Vinyals and Le. 

## Notes
1. There's so much mystery within this paper. For example, the model performed better when the input sequences were given in reverse. The authors weren't aware at the time why it was that this was happening - maybe I'll find the answer in later investigations. 
2. Interestingly, the perplexity noticeably decreased with deeper architectures. Deeper architectures appear key to handling genuine complexity. This could be a design principle: if your RL agent plateaus in the sophistication of it's strategies, try making it's architecture deeper.
3. I do need to recap long-short-term memory again - it's been over a year since I studied it for the first time. I'll probably do that tomorrow.

## Remarks
1. Today was a mess. I worked through my lunch break: this is when I usually read papers. I also came out of work late today, so my evening was a bit packed. 