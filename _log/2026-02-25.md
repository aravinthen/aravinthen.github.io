---
layout: log
title: I'm workin' on it!
date: 2026-02-25
math: True
tags: [reinforcement-learning]
---

## Summary
* Read *Policy Gradient Methods for Reinforcement Learning with Function Approximation* by Sutton et al. 

## Notes
1. This is an interesting paper from a historical perspective - there was a point where only value functions were approximated. The list of limitations of value function approximation is illuminating:
    * Value functions work well for deterministic policies, but the optimal policy is often stochastic. Kind of like how a Nash equilibrium may not exist, but a mixed Nash equilibrium always exists?
    * Discontinuous behaviour arises in the value function update as a result of small arbitrary changes.
    * Oh wow: Q-Iearning, Sarsa, and dynamic programming methods don't actually converge when you use function approximators, even for simple problems. That's good to know!
2. It makes sense now why policy gradient methods are stable: changes in the weights can only really cause small changes to the policy. Granted, I think there's a clear complication that arises here when you introduce actor-critic algorithms: whilst the updates might be small, it's important to make sure that the actor doesn't change too quickly.
3. `REINFORCE` learns much more slowly than methods with a learned value function - got it!
4. **Advantage function**: $$A(s, a) = Q(s, a) - V(s)$$. A measure of how much better an action is than the average of the action of a state. This emerges naturally from the formulation of `REINFORCE` with baseline, which is essentially what this paper is introducing.  The advantage function *still* manages to estimate a positive direction for the policy (and so drive the policy to an optimum), but it does so with much less variance! 
5. Advantage functions work by isolating the result of an action, which in turn allows for more appropriate signalling for learning. This leads to better credit assignment. 

## Remarks 
* Spent 11 hours at work today. I've made progress at work, but less progress on reinforcement learning! :( 