---
layout: log
title: Attention paid off
date: 2026-01-25
math: True
tags: [deep-learning]
---

## Summary
* Finished reading [The Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/). 
* Wrote up my literature review for the week.

## Notes
### The Illustrated Transformer
1. I decided to supplement my reading of *Attention is all you need* with a less research-paper based approach. This was a good idea! I'm realising that my issue isn't so much the fact that I don't understand what's going on - I'm having trouble understanding the hierarchy that emerges when you put the pieces together.
2. Let's walk through the whole architecture in terms of hierarchy. The transformer architecture is essentially composed of an encoder stack and a decoder stack.
    1. The **encoder stack** comprises of 6 encoder layers. These are not like RNNs - each of these layers has different weights. The encoders are identical in structure and consist of a **self-attention** module and a **feedforward network**.
        1. The self-attention layer allows the encoder to look at the other words in the sentence as it encodes another word. The key conceptual idea here is to realise that a word alone doesn't possess sufficient context in most situations to fully describe what it actually means. Just translating a word isn't enough to fully describe its meaning - the self-attention layer seeks to provide a *true* representation of the meaning of that word by relating it to its surroundings. The goal here is to build better, more complete word embeddings. The process by which self-attention is calculated is the following:
            * Three vectors are created from each of the encoder's input vectors: the key (K), the query (Q) and the value (V). The vectors are each created from the original inputs via a differentiable operation with tunable weights. **Note**: this process is done multiple times based on the number of attention heads are present within the self-attention layer.
            * You calculate the *score* by taking the dot product of the key and query vectors. The score is a way of calculating the "distance" between the embeddings - it tells us how much each embedding matters to each other.
            * You stabilize the score by dividing it by the square root of the dimension of the key vectors. You then apply a soft-max and then multiply the result of the soft-max to the value vector. This is the step in which irrelevant word pairs are drowned out of the final calculation.
            * The weighted value vectors are then summed together. Only the relevant components of the embedding are left. 
            * This is an example of *one* head of attention. As it happens, transformers have many heads of attention where the heads themselves are represented by a *set* of multiple key/query/value weight matrices. The transformer has eight sets of key/query/value matrices, which means that it has eight attention heads. This is done because a transformer benefits from multiple representation subspaces. Essentially, we carry out the previous steps multiple times for multiple weights.
            * *As a result of having multiple attention heads, you need a way to combine the output of each attention head into a single output*. You do this by concatenating the output of each attention head and multiplying that output with a trainable weight matrix that sets the output into the correct representation. The result is a single word embedding that takes context from all of the attention heads. *This* is what is sent to the feedforward layer.
        2. The feedforward neural network, which first runs the output of the self-attention layer through an additional and normalization layer. These are *residual layers*: the goal of these components is to allow for deep, stable training to prevent information loss and vanishing gradients. 
    2. The **decoder stack** comprises of 6 decoder layers. The layers of the decoder are quite similar to the encoder, but with an addition layer in the middle. That is, **self-attention**, **encoder-decoder attention** and a **feedforward network**. The input to the decoder stack aren't *just* the word-embedding calculated in the final encoder layer: rather, the output of the encoder layer is is transformed into a set of key and value vectors. **Note**: The key and value vectors are passed into the decoder layer *in addition to the un-encoded positional word embeddings that were also passed into the encoder*.
        1. The self-attention layer, which works exactly like the self-attention layer in the encoder.  
        2. The encoder-decoder attention layer, which also basically works in the same way as the self-attention layer preceding it. *However*, unlike the self-attention layer, the encoder-decoder attention layer uses the query matrices from the self-attention layer below it and uses the key/value vectors that are provided from the encoder.
        3. The feedforward neural network, which like before is preceded by sublayers that are intended to stabilize the training process. 
    3. The decoder stack outputs a vector of floats. A final linear layer is used to transform the output of the decoder stack into a vector that is as large as the vocabulary of the model that is being trained. The output of this layer is called a *logits* vector, which essentially determines a score for the word represented by the index of the logits vector. Finally, a soft-max layer is used to translate that logits vector into actual output probabilities for the next word in the sequence. 
3. A note on inputs: an embedding algorithm is used to provide an encoder layer a list of *word embeddings* of length 512. Each encoder layer will subsequently provide a list of embeddings of the same length. Note that the length of the list is a hyperparameter, but is usually set to the longest sentence within the training dataset.
4. *The word in each position flows through it's own path in the transformer*. That is, the tokens are *not* natively processed in a sequential way: they're provided as a single list to the encoder layer. This is a major strength: every path that a word might follow is independent, which also makes transformer processing easily parallelizable. It's worth mentioning that the original paper, the word embeddings are directly provided a positional encoding to allow them to be handled in terms of the order within the sentence. Note that the positional encoding isn't some trivial process - you can't just "add the index" to the representation. 
5. The sizes of K, Q and V are constant and smaller than the 512 length word embeddings.
6. V has a pretty standard interpretation a scoring mechanism for context. On the other hand, Q and K are a little more subtle. Q represents what the current input token is looking for while the key acts as a label or "context" for what *other* tokens offer. This is analogous to how retrieval systems operate (where a query is mapped against a set of keys.
 
## Remarks
* I still don't feel so good, but not in a way that I couldn't work. Today was a productive day. :)  
* Anyway, I think I actually understand transformers now. Awesome! The relevant information is within my brain now - I suppose I'll have to think about this constantly for the next few weeks until I develop an intrinsic understanding of what is going on here. :) 
