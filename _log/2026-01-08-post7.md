---
layout: log
title: Environments 
date: 2026-01-08
tags: [reinforcement-learning]
---

## Summary
* Finished reading [Debugging RL, Without the Agonizing Pain](https://andyljones.com/posts/rl-debugging.html?utm_source=chatgpt.com) by Andy Jones. 
* Started reading Chapter 3 of Sutton and Barto. Nice being back into the thick of some theory!

## Notes
1. One of the useful insights gleaned today is that the environment should never be a static variable: multiple environments should be actively used to probe performance. There could be a hierarchy of environments that *gradually* progress to the full environment - not just as a form of curriculum learning, but as a means of testing functionality.
2. The opposite process should be taken with the agent. Engineer a very strong agent (which has either perfect information or at the very least more information that the intended agent) and degrade it until you achieve realism.
3. Really, really useful section on logging. This will be included in the literature post for this week.
4. Markov decision processes are *still* a limited version of the full reinforcement learning problem. They fall flat when you need to consider memory and context, although I'm guessing there'll be an elegant way to encompass these into the state variables of the current state.

## Remarks
1. I wonder if tunable discretization is a viable means of gradually developing a realistic environment. Say a realistic representation of your environment requires a very large number of grid points. If you start off with a lower number of grid points, train your agent on that and them progressively make the grid finer, what qualitatively distinct features might emerge with this finer detail (if any)?
2. It's really, really nice to be doing probability theory properly again. Sutton and Barto makes it way easier than other rigorous treatments, too. :) 