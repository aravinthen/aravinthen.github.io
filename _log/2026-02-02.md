---
layout: log
title: Too graphic for me
date: 2026-02-02
math: True
tags: [deep-learning]
---

## Summary
* Read *Semi-supervised classification with graph convolutional networks* by Kipf and Welling.
* More work on Sutton and Barto.

## Notes
1. Back to graph neural networks. As usual, this is a considerably complex form of architecture... and we're about to make it even more complex with the introduction of convolutions.
2. This paper is my first time considering the problem of **semi-supervised learning**. Semi-supervised learning is a hybrid approach where labelled data and unlabelled data are *both* used to train networks for classification and regression.
    * You typically used semi-supervised learning when you have an abundance of unlabelled data *as well as* a bit of labelled data.
    * This is great for me, because I am very lazy. This means that I can label a few cases of my dataset and then train regardless of whether or not I labelled all of my data points.
3. The semi-supervision comes from the fact that only a few of the nodes in the graph are actually labelled. We're effectively operating upon the assumption that if two nodes are connected by an edge (or have high edge weight/similarity), they are likely to share the same label. The problem is to find the operator that carries out the assessment of this likelihood.
4. The feature vectors and adjacency matrix of the graphs being trained are used as input directly into a neural network. This is a little different from the basic formulation from Gilmer et al., although it can almost certainly be represented in terms of the message, update and readout framework described. 
5. The graph convolutional step described in the equation actually combined the message passing and update step. 
6. Oh god, the maths. I'm out of my league here, but from what I gather,
    * A spectral convolution is when you multiply the node signals with a filter in the Fourier domain using the eigenvectors/values of the graph Laplacian (the degree matrix minus the adjacency matrix). 
    * To handle the fact that this is computationally expensive, Chebyshev polynomials (which are typically used to approximate filters in electronics) are used to approximate the graph filter. 
    * This is essentially no different to carrying out a convolution over graphs, as truncating the polynomials to first order is essentially the same as localizing the filter to a single neighbourhood.
7. In essence, the differentiable function that you're trying to train is the the filter and the convolutions basically just fall out of the approximation. 
8. Alright, so after multiple applications of the standard convolution operator, you'll typically run into a situation where the eigenvalues of the operator cause exploding/vanishing gradients. The renormalization trick is applied to constrain the eigenvalues to 0-1, which stabilises the training process. 
9. The readout network is provided in (9).
10. The results of this paper are *dramatic*. The inference process here is a lot faster and more accurate! It *is*, however, noted that the formulation doesn't work for directed edges and edge features.
 

## Remarks
* Oof. This week's paper was tough. I've spent a bit of time handling graphs so covering the paper wasn't *so* bad, but this is an indication that I should be reading more on graphs...
* I wanted to spend this evening implementing a transformer. Unfortunately, this is too large a task to just squeeze into a single evening without significant vibe coding. I don't want to vibe code this: I need to learn how to do it from scratch, because that is the only way that I'll learn.
* Fell asleep reading Sutton and Barto. This is why I forgot to push!