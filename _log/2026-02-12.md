---
layout: log
title: Lighting the Torch(RL)
date: 2026-02-12
math: True
tags: [reinforcement-learning]
---

## Summary
* Started reading *Torch-RL: A Data Driven Decision-Making Library for PyTorch* - Bou et al. 
* More work on Sutton and Barto - blazed through Chapter 7.

## Notes
* I can see that this is by far the most recent of papers that I've regarding the RL ecosystem, having been submitted on the `ArXiv` in 2023. It's still three years old, but the fact that it even exists is either indication that other solutions developed since then have their own limitations... 
* It's very interesting to see that the authors consider RL in a control-theoretic sense. This is, to the `TorchRL` devs, a system for pure decision-making agents. I quite like this emphasis!
* Ah, the diagram is really good. It also emphasizes how easy it is to use this stuff in a modular way. I'm guessing however that the main barrier to adoption is the heavy use of `PyTorch`, as well as the fact that modularization necessarily requires some level of expertise in the problem at hand.
* "We believe that moving forward, the next generation of libraries for decision-making will need to be easy to use and rely on widespread frameworks while still being powerful, efficient, scalable, and broadly applicable, to meet the demands of this rapidly evolving and heterogeneous field. This is the problem that TorchRL addresses." **Awesome**.
* The essence of `TorchRL` appears to be the main data communicator object - this can also be seen on the main diagram. The emphasis on `TorchRL` is the flexibility that each component demands. 
* Holy shit - `TorchRL` introduced `TensorDict`. That's awesome. I had a coworker tell me today that `TensorDict` was built independent of `TorchRL`, but it just so happens that this isn't true: it's actually that `TensorDict` has just grown into it's own tool. This is **very** encouraging...
* For some reason, `RLLib` is listed as a *high-level* approach similar to `Stablebaselines3`. This is quite interesting to me. I suppose that the essence of this comparison is based around the fact that algorithm-dependent components are offered as part of `RLlib`.

## Sutton and Barto
1. I've developed some level of understanding on the basically methodological behind RL. Nothing in this chapter was particularly surprising to me, which is why I was able to read it all in a single evening.
2. A lot of this makes sense in a computational physics perspective, too: timesteps are merely sampling points throughout the trajectory. It makes sense that methods that can sample over many series of timesteps can inform bootstrapping significantly better in situations that are sensitive to sharper changes. Further, the treatment of the delayed value function $$V_{t+n-1}$$ is also very much an error estimate that you'd typically see in numerical analysis.
3. I mean... a lot of this chapter is quite intuitive, no? It's a middle ground between MC and TD(1), with the addition of importance sampling as a crucial addition for off-policy methods.

## Remarks
* Ran out of time to read the paper today - had to take the train back to my workplace after my work event. A shame, too: this paper is quite nice. I like it.
* Skipping Chapter 8 - the summary is sufficient for my paltry requirements and I've already read a paper that details the a representative of this class of methods (and that actually used function approximation!).
* A useful piece of terminology from the Chapter 8 summary: "**A sample model**, on the other hand, is what is needed to simulate interacting with the environment duringwhich sample updates, like those used by many reinforcement learning algorithms, can be used. Sample models are generally much easier to obtain than distribution models."
* In a sense, a world model is basically a sample model in a way... 
* The graphic at the summary of Part I of the book is awesome! Thinking about the methods as *depth* of search versus *width* of search makes perfect sense. :) 