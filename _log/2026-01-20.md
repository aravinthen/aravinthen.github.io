---
layout: log
title: Starting to pay attention 
date: 2026-01-20
tags: [deep-learning, reinforcement-learning]
---

## Summary
* Finished *Neural Machine Translation by Jointly Learning to Align and Translate* by Badanau et al. 
* Started reading *Effective approaches to attention-based neural machine translation* by Luong et al. 
* More progress on Sutton and Barto.

## Notes
1. It's really, really cool to see how alignment is measured. The way that context spreads through the input source string - especially from French, where the determiner is gendered - is awesome. I wonder what the alignment would like if the decoder was trained on Japanese?
2. Encoding - (ranking - decoding): this is the essence of the approach. The ranking itself is a weighting over likelihood. Very, very powerful - granted, the architecture described here is specialised for sequences of up to 50 words.
3. Neural machine translation demonstrate the strength of the AI approach: neural decoders are *easy* to engineer, whereas rule-based systems can quickly become extraordinarily difficult and complex.
4. **Attention mechanism**: "a set of source hidden states which are consulted throughout the entire course of the translation process". 

### Sutton and Barto
1. I need to get mathematical notation set up on this site so I can post some of my solution notes. :( 
2. Policy iteration is really familiar to a lot of the RL stuff that I've already done. It's a concept whose basic architecture appears to be a stable point in the field of reinforcement learning. Just as a recap:
    1. Initialise your system with a randomized value function. 
    2. Carry out policy evaluation, where you iteratively improve your value function by updating each state.
    3. Carry out policy improvement, where you
        * Generate an action with the old policy,
        * Generate an action with a new policy,
        * Compare the value of the new action *with respect to the value function of the new policy*.
        * Accept the new action as the standard policy if it yields a higher value than the old action.
3. The last step in the previous point is a general result known as the *policy improvement theorem*. 

## Remarks
* Seeing the way in which one basic problem is continually and incrementally improved is thrilling. I started with RNNs last week and now I'm thinking about the differences between local and global attention! 