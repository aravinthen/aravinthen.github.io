---
layout: log
title: Diving into the RL Ecosystem
date: 2026-02-10
math: True
tags: [reinforcement-learning]
---

## Summary
* Read [Stable-Baselines3: Reliable Reinforcement Learning Implementations](https://araffin.github.io/post/sb3/) by Antonin Raffin. 

## Notes
1. The essence of `Stablebaselines3` is a clean, simple interface for running baselines implementations of common Rl algorithms. 
2. The library is impressive in that a great deal of functionally can be incorporated into just a few lines of code. The example on Raffin's blog is really nice:
    1. You make the environment,
    2. You define the model, which in this case is a full algorithm class that encompasses the full machinery of the approach. Everything is determined in here, from the architectures being used to the algorithms employed to train the architectures. 
    3. You set the algorithm object to learn in a single line, specifying only the timesteps.
    4. Loading and environment control are also implicitly enabled.
3. **The essential motivation of this library is to provide a means of baselining in a user-friendly and reliable way**. The algorithms in the library are extremely well tested.
4. The library focuses on a particular class of algorithms: model-free, single agent DRL. In fact, it appears that outside of defining architectures and customizing the attributes, it is not possible to modify the algorithms too extensively. Basically, this is not a building block library.
5. The blog post *explicitly* mentions that the software is meant to readable over modularizable. 
6. There's something confusing here in the work flow: the algorithm and the agent are referred to interchangeably. I suppose this is based on the fact that, as computational objects, they *are* the same thing. However, I've seen many instances in distributed RL systems where having a distinction between the agent and the learner is important. Interesting...
7. ... and it's because `Stablebaselines3` is not designed for distribution. That makes sense: distribution is a massive pain and this is not meant to be a library primarily for RL experts.
8. The reproducibility results are really impressive. They aren't *identical* - this is to be expected.
9. One of the huge strengths of this library is that, because all of the difficult algorithmic features are basically abstracted out, it comes with a lot of features for intepretibility. The system works *really* well with Tensorboard.
10. Okay, there's some level of customizability within `Stablebaselines3` when it comes to customizing architectures. I'm not sure, however, how useful this would be if you want to develop large and complex agents that make use of world models and the like. Indeed, given that *so much* of the detail is encapsulated in the algorithm/agent class, it's actually remarkably difficult to build out of it.
11. **Verdict**: This is a good library to start exploring reinforcement learning algorithms and provides a very wide variety of avenues by which to *solve environments*. In *World models* by Ha and Schmidhuber, solving an environment has a very specific criteria. I can see how `Stablebaselines3` provides all the necessary tools to meet these solution criteria.
    * However, it's pretty clear that this is not a research-standard library for anything outside of benchmarking and evaluation. I can see why someone who is simply interested in environment development would be attracted to this software, but I don't think that this is a library that would be suitable for developing cutting edge reinforcement learning approaches... *and that is fine*. Going back to Ha and Schmidhuber, it would be very difficult to implement their approach in `Stablebaselines3`, because it's not really meant to explore new RL approaches.
    * The software is very, very good for exploring paper-implementations of libraries in that the difficult part of *implementing a paper* has been abstracted out. In general, this is a good starting point for seeing if your environment is a good fit for reinforcement learning.
12. I had a quick look at whether `Stablebaselines3` support MARL. Funnily, the Petting-Zoo environment explicitly warns users that `Stablebaselines3` is not designed for MARL. Indeed, the workarounds to this invariable force you to make a single agent that controls multiple actors. 
 
## Remarks
* I have decided to spend a bit of time deeply assessing the current ecosystem of RL libraries. I feel like having a finger on the pulse of used systems is probably a good step, *because I do not want to be reading RL theory forever*. I'd like to move on to actual, practical research engineering as soon as possible.
