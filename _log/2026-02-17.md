---
layout: log
title: Graph confusional networks
date: 2026-02-17
math: True
tags: [graph-neural-networks]
---

## Summary
* Finished reading *Relational inductive biases, deep learning, and graph networks* by Battaglia et al. 

## Notes
### Literature
1. One interesting feature of GNNs is that they're stated in the paper to have a stronger relational bias than CNNs and ANNs. I wonder why this is the case? Could this be formally shown as per something like the Weisfeiler-Lehman test?
2. Here's the kicker: *"Recently, Gilmer et al. (2017) introduced the message-passing neural network (MPNN), which unified various graph neural network and graph convolutional network approaches by analogy to message-passing in graphical models."* 
3. So that's the difference. The *Graph Network* approach is something of a generalized approach, or a methodology, that the message-passing networks are a special case of. They aren't so much completely different architectures at all.
4. In summary,
    * Message Passing Neural Networks are framework where nodes send messages along edges; messages are aggregated at neighbors and used to update node features.
    * Graph networks are a strict formalization that extends MPNNs by always incorporating edge updates and incorporating global context. Further, every component of a graph network is a learnable function!
5. The essence of a graph network block is pretty much just a graph-to-graph mapping. The relational inductive biases come from explicitly defining learning in terms of node/edge update functions and aggregation functions. There *is* an underlying architecture to these graphs...
6. The paper mentions that graph networks support a form of combinatorial generation because the learnable functions that represent their update and aggregation functions are reused over all edges and nodes. I saw this in action in the paper by Sanchez-Gonzales et al. - I'm not particularly sure that this is really combinatorial generation for pthe reasons I discussed](https://aravinthen.github.io/2026/01/10/literature_review/) when reading that paper. 
7. **Why don't the message passing networks discussed in Gilmer et al. exhibit this kind of combinatorial generalization?** Is it because they work in a way that stresses locality? No, that can't be it... I need to give this some thought. Will return to this in the literature review for this week.
8. So, Section 4.2.1 discusses how MPNNs can be written in the form of the graph network formalism. Just by the way that this is done demonstrates that MPNNs are weaker in principle than the fully general formulation of the GN. But still, why does this weakness cause the loss of the combinatorial generalization?
9. *"Although we are excited about the potential impacts that graph networks can have, we caution that these models are only one step forward. Realizing the full potential of graph networks will likely be far more challenging than organizing their behavior under one framework..."* - yup. GNs are extremely general objects. So general that all of the papers that I've read so far about GNNs in some way can be tied back to them.

## Remarks
* I am **tired**. I've been waking up at 5:30AM today to try and switch my working hours to an earlier time. I'm by and large a little more productive, but I'm still a bit sleep deprived. I'm going to tuck in early today. 
* That's the weird bit about discipline. You've got to at some point accept that you'll have less productive days - the only thing that matters is if you show up.