---
layout: log
title: The fiftieth day
date: 2026-02-20
math: True
tags: [deep-learning]
---

## Summary
* Read [An introduction to Graph Transformers](https://kumo.ai/research/introduction-to-graph-transformers/) by Lopez, Fey and Leskovec.

## Notes
### Literature
1. Let's hone in on what's going on with graph transformers. 
2. Transformers are essentially machines that create a fully-connected graph between the contents of their input tokens, then learn the appropriate importances between those tokens. Self-attention is merely a means of aggregating information from the feature vectors corresponding to each token.
3. **Oh**. No, that makes sense. I get where the positional encodings come into this. The eigenvectors of the graph Laplacian are used to encode the relative position of nodes from each other. The issue was that before I thought that graph transformers and transformers has analogous position encodings - the truth is that the position encodings of the graph transformer capture *relative position* whereas the positional encodings of the sequence transformer captures *sequential* position.
4. Then there's the fact that graph transformers incorporate edge-specific information into their attention calculations - this was clear even in the previous paper.
5. It's worth mentioning that the inductive biases inherent to the graph transformer manifest in a few benefits, including
    * More appropriate access to relevant parts of the input,
    * Lower computational complexity 
6. Of course, some systems can't be easily reduced to graphs. Sentences are a good example of this - this is what the paper from yesterday was noting, where sentences by themselves are very sparse in terms of context.
7. Some problems that I was aware of with graph neural networks and how graph transformers solve them:
    1. Oversmoothing, where deeper graph neural networks tend to pass so much information throughout the network during message passing that after a while all the nodes start to look the same. According to the blog post, graph transformers defend against this by being able to control which nodes inform the aggregation via attention.
    2. Oversquashing, which basically means that too much information is forced into the update layer. This is a consequence of heavy connectivity - if every node connects to every node, you're going to get too much information to effectively aggregate necessary values. This is also fixed by the attention mechanism.
8. Ah, but there's a kicker here. Graph transformers find it challenging to handle very, very large graphs. I daresay that this is a basic limitation for the graph network framework discussed in Battaglia et al., too...
9. Some of the graphs in this blog post were taken from another paper, *Recipe for a General, Powerful, Scalable Graph Transformer* - this has been added to my reading list. :)    

## Remarks
* A quieter day today. I'm in the process of trying to set up an academic collaboration at my work place with a former colleague of mine. I have a packed weekend, but regardless there will be at least some output. 