---
layout: log
title: Environments 
date: 2026-01-09
tags: [reinforcement-learning]
---

## Summary
* Had a deep study session with Sutton and Barto.

## Notes
1. The boundary between the agent and the environment is by no means fixed - it can be moved at will. 
2. I fell into a trap today: I forgot that many situations are *partially observable* i.e. in poker, where you can't see opponent cards. This disqualifies the basic Markov decision process as an appropriate modelling approach. This is something that people reading S&B after exposure to SOTA methods might have to get used to.
3. Everything is in some way a generalisation of finite automata. This makes me happy, because I like finite automata. :) 
4. Some of this stuff is pretty deep in how clinically it treats very relevant subject matter to human experience. "All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received
scalar signal (called reward)."

## Remarks
1. Got no sleep last night. Probably a sign that I have to start being more disciplined with my sleep schedule. As such, I decided to just focus on Sutton and Barto today. The mathematical sophistication here is somewhere around the level of an undergraduate textbook, so it's quite easy getting through it.
2. I once knew someone who had taken a well known psychedelic known as *salvia divinorum* a number of times. Something that he mentioned a couple of times was the apparently very jarring sensation of losing the ability to distinguish between his body and the world around him, with the boundary of "him" and the "rest of the universe" vanishing at points. That's sort of what the agent/environment definition reminds me of.