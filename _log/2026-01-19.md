---
layout: log
title: Aligning and translating literature
date: 2026-01-19
tags: [deep-learning, reinforcement-learning]
---

## Summary
* Started reading *Neural Machine Translation by Jointly Learning to Align and Translate* by Badanau et al. 
* Started Chapter 4 of Sutton and Barto. 

## Notes
### Badanau et al. 
1. Further down the path of the transformer. 
2. The encoder is a bidirectional recurrent neural network. *What the hell is a bidirectional recurrent neural network*???
    * Alright, so it's basically just a recurrent neural network that processes data both in the forward direction and the backwards direction, simultaneously.
    * Both networks predict the output, but one reads the input sequence in from T=1 to T=N, whereas the other network reads the input sequence from T=N to T=1.
    * The results of both networks are combined so that the output understands the full context of the sequence. 
3. **Annotation** is where you convert the raw input tokens of a phrase into a series of richer, contextualized vectors. An annotation implicitly contains information about the whole input sequence, but *focuses* attention on the parts surrounding the part of the input that it corresponds to. In the paper, annotations are generated by the encoder - they're built from the bidirectional RNN by concatenating the forward and backward passes for each timestep. 
4. **Alignment** is the process of establishing correspondences between words in the source sentence and their translations in the target sentence. This process is carried out in the decoder via a gated recurrent unit.
5. The relationship between annotations and alignment is basically the following:
    * Generate a sequence of annotations using a bidirectional RNN.
    * Build a context vector as a weighted sum (softmax) of the *alignments* of the annotations. 
6. It's worth mentioning that the alignments are essentially *scores* that can be used to rank the aforementioned correspondence. The alignment model that you use can be a feedforward neural network (as in the case of this paper), which is called *soft* alignment due to the fact that the model is a probability distribution over the source sentence. 
7. **Attention** emerges when the decoder decides which parts of the source to pay attention to. And there we have it!
8. Like in Sutskever et. al. (2014), beam-search is used to maximise the conditional probability of the decoder. 

### Sutton and Barto
1. "The key idea of DP, and of reinforcement learning generally, is the use of value functions
to organize and structure the search for good policies." Value functions are the key tool of reinforcement learning. In a sense, I'm starting to realise that reinforcement learning is nothing but the engineering and calculation of an appropriate value function. 
2. The prediction problem: calculating the value function of a given policy. 
3. I spent a bit of time investigating why iterative policy evaluation works. The main reason is that the Bellman equation is actually an operator on the policy - the operator itself is a contraction depending on the discount factor applied. Every time the Bellman operator is applied to a policy, the value function approaches its fixed point. Elegant!

## Remarks
* I used AI to correct my understanding of what a BRNN is - I basically just typed out the above paragraph, fed it into an AI and checked if my understanding was right. It wasn't, so the AI told me where and why I was wrong. Now, I understand the concept. Awesome use-case!
* The weight of the annotation is called the "energy" in the paper. Interesting, although it's unclear whether there's a clear correspondence with energy as you'd see it in ensemble theory.
* Didn't manage to finish the paper today - spent too much time getting up to date with bidirectional RNNs. It's all good, though - I'm learning a *ton* here.