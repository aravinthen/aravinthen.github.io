---
layout: log
title: How long is this paper?!
date: 2026-02-16
math: True
tags: [graph-neural-networks, reinforcement-learning, binary-search-trees]
---

## Summary
* Started reading *Relational inductive biases, deep learning, and graph networks* by Battaglia et al. 
* Spent a bit of time going over binary search trees.
* Started my reinforcement learning project by writing half a Markov decision process simulator.

## Notes
### Literature
1. This is probably one of the longer papers I've read. I can already tell that it might take me *two* lunch breaks in order to read it.
2. Another Google Deepmind paper. It's probably worth mentioning that it's pretty clear that Google Deepmind is probably the world leader for their use of graph neural networks. I should probably write a review blog post on the applications of GNNs soon.
3. I've learned a very important term today: *combinatorial generalization*, or the ability to create a myriad of complex structures from a finite number of building blocks. 
4. This paper was written before the advent of complex language models - it states that *deep learning approaches are not good at combinatorial generalization*. Is that still true? I think that language models *do* show quite a remarkable ability to generalize given a set of building blocks (in this case, tokens), but I'm not yet convinced that language models are truly capable of combinatorial generalization. When was the last time an LLM verifiably came up with a completely new idea?
5. The authors of this paper *very* specifically define structure as the product of composing a set of known building blocks. "Relational reasoning, then, involves manipulating structured representations of entities and relations, using rules for how they can be composed."
6. This makes sense, but doesn't this presuppose the architecting of a specific structure beforehand? You're not really *learning* a structure if you're employing relational inductive biases...
7. There's a very strange kind of generality here that I'm having a bit of trouble digesting. There is *clearly* a distinction between the graph neural networks discussed in Gilmer/Kipf/Hamilton/so on with the kinds of graph networks that Deepmind use. But what is that generality, really? Indeed, I'm at a point of confusion where I'm not even particularly sure *who* generalizes *what*!
8. The definition of inductive bias is very useful here: "An inductive bias allows a learning algorithm to prioritize one solution (or interpretation) over another, independent of the observed data."
9. Ah... the basic inductive biases of the stacked layer approach to deep learning is that of hierarchical processing, where components of the inputs are assumed to have a long-range interaction.
10. In a sense, an inductive bias is merely a constraint on the trajectory of the learning process. Furthermore, thinking of inductive biases in terms of MLPs being a basic *low inductive bias* system allows one to imagine the way that other deep learning architectures progressively demonstrate an increase in some level of inductive bias...
11. This is such a good paper - will be finishing it tomorrow. I simply ran out of time today!

### Binary search trees
* Will I ever use binary search trees at work? Honestly, I'd be surprised if I do. However, I'm getting a feeling that the *depth* to studying algorithms like this isn't so much the use of them. Rather, it's the ability to sniff out useful properties of a data structure and use it in problem solving. 

### Deep RL experiments
* I've decided to write some classic reinforcement learning algorithms, just to get into the swing of things (and so that the last month I've spent reading theory isn't wasted). I think that easing myself into the project and getting into the flow of doing **experiments** is essential for mastering this field.
* What is the point of doing classic reinforcement learning when I want to move onto DRL? The essence of it is that, just by observing the structure of the RL problem, I can tell that using deep learning models is going to make *everything* harder. Let's keep this easy and draw out the structure first, hm?