---
layout: log
title: The whole wide world
date: 2026-01-30
math: True
tags: [graph-neural-networks, software-engineering]
---

## Summary
* Read *Neural Message Passing for Quantum Chemistry* by Gilmer et al. 
* Made more progress on *Effective Python*. 
 
## Notes
1. There was a point in my life where I was taking a few quantum chemistry courses. Let's see if I remember anything.
2. The general framework that is being described in this paper is that of 
    1. Message passing,
    2. Node/Edge updates,
    3. Readout.
3. The above is a really generalizable form of thinking about graph neural networks. You can in some sense think about this as the "recipe" for graph neural network applications.    
4. There's a really interesting means of parallelizing the graph neural networks described here - the paper refers to the technique as "multiple towers" and demonstrates a factor of 2 speed up. Turns out that breaking things into towers helps the generalization process, too.
5. Message, update, readout. 
6. This was a really useful paper to demonstrate actually *using* graph neural networks. In general, the *readout* step is the foundational aspect here in translating the graph that you've generated... and makes it clear how the general methodology would correspond to graph-to-graph/graph-to-feature mappings.
7. Message, update, readout!!! 

## Remarks
* Ah, crap. Forgot my kindle again. At this rate it'll take me about seven months to finish *War and Peace*!
