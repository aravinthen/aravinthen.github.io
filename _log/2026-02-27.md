---
layout: log
title: Tripped up by the maths
date: 2026-02-27
math: True
tags: [reinforcement-learning]
---

## Summary
* Started reading *Deterministic Policy Gradient Algorithms* by Silver et al. 

## Notes
1. ... there was a point that people didn't think that model-free deterministic policy gradients existed? Interesting.
2. "In the stochastic case, the policy gradient integrates over both state and action spaces, whereas in the deterministic case it only integrates over the state space. As a result, computing the stochastic policy gradient may require more samples, especially if the action space has many dimensions". This is because the stochastic policy gradient necessarily has to generate actions in order to sample states in the first place, whereas deterministic policy will always just do the same thing.
3. The deterministic policy gradient algorithm in this paper is *off-policy*. As such, a stochastic policy is used for sampling whereas the deterministic policy is the result of the training process.
4. Interesting: the performance objective here is written as an integral. This is a little different to the version described in Sutton and Barto, although you can see how the value fucntion corresponds here.
5. Got tripped up with following the mathematics here and ended up running out of time on the train. I'll be finishing this paper tomorrow!

## Remarks  
1. I am so glad that I spent time going through Sutton and Barto. I now realise that I have a really nice framework to think about reinforcement learning problems. 
2. I realised today that I'm diverting my attentions a little too much. I have decided to switch entirely to focused projects in order to learn new things. I'm currently attempting some sort of flux between reinforcement learning and deep learning - in the process I'm neglecting both fields but not focusing the extensive time that I have on the weekends on the former and relegating the latter to two days that are already hard enough for me to keep consistent with.
3. As such, I'm going to focus on deep reinforcement learning entirely. The deep architecture stuff will come when it comes! :) 