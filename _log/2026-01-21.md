---
layout: log
title: Paying too much attention 
date: 2026-01-21
math: True
tags: [deep-learning, reinforcement-learning]
---

## Summary
* Finished reading *Effective approaches to attention-based neural machine translation* by Luong et al.  
* More work on Sutton and Barto.

## Notes
1. The basic difference between Bahdenau and Luong is the introduction of the window of localized attention. I note that the window size is empirically selected: this was the main mental block for me when reading the paper, as I was implicitly assuming that it was a parameter.
2. Adding detail to a passage from the paper,
    * The model first predicts a single aligned position for the current target word, where alignment is the process of establishing correspondences between words in the source sentence and the target sentence.
    * A window centered around the source position is then used to compute a context vector, which is a a weighted average of the source hidden states in the window.
    *  The weights at are inferred from the current target state and the source states included within the window.
3. The distinction between monotonic alignment and predictive alignment is interesting. The aim of both is to find a "center point" within the sentence, but monotonic alignment represents a one-to-one correspondence between the source sentence and the output sentence whereas predictive alignment attempts to infer which source input is most appropriate. 

### Sutton and Barto
I normally scrawl my solutions on a notepad, but I'm going to include my solution for Exercise 4.5 here given how important it is to the rest of the textbook. It'll also provide a testbed for the `mathjax` that I just set up. :)

**Question**: How would policy iteration be defined for action values? Give a complete
algorithm for computing $$q_*$$, analogous to that on page 80 for computing $$v_*$$. Please pay special attention to this exercise, because the ideas involved will be used throughout the
rest of the book.

**Solution**:
Let's recall the policy algorithm described in the previous log. First initialise $$V(s) \in \mathbb{R}$$ and $$\pi(s) \in \mathcal{A}(s)$$ arbitrarily for all $$s \in \mathcal{S}$$.

We then carry out policy evaluation, where 

$$
\hspace{0pt}
\begin{array}{l}
\text{Loop:} \\
\qquad \Delta \leftarrow 0 \\
\qquad \text{for each } s \in \mathcal{S} \text{ do} \\
\qquad\qquad v \leftarrow V(s) \\
\qquad\qquad V(s) \leftarrow \sum_{s',r} p(s', r \mid s, \pi(s))
  \left[ r + \gamma V(s') \right] \\
\qquad\qquad \Delta \leftarrow \max(\Delta, |v - V(s)|) \\
\text{until } \Delta < \theta
\end{array}
$$,

where $$\theta$$ is a convergence parameter. Following this, we carry out policy improvement where

$$
\hspace{0pt}
\begin{array}{l}
\text{policy-stable} \leftarrow \text{True} \\
\text{For each } s \in S: \\
\quad \text{old-action} \leftarrow \pi(s) \\
\quad \pi(s) \leftarrow \arg\max_a \sum_{s',r} p(s', r \mid s, a) \left[ r + \gamma V(s') \right] \\
\quad \text{If old-action} \neq \pi(s) \text{, then policy-stable} \leftarrow \text{False} \\
\text{If policy-stable, stop and return:} \leftarrow V \approx v_*, \pi \approx \pi_*,\\
\text{else, go to 2}
\end{array}
$$

In order to change this formulation to apply for action-values, we have to make the following changes
* Replace the value function $$V(s')$$ with $$Q$$ throughout, which also means replacing the value function for the next state with the appropriate sum over the actions and the $$Q$$ function.
* Sum over the possible actions as well as the state.

The above formulation will then be written as
 
$$
\hspace{0pt}
\begin{array}{l}
\text{Loop:} \\
\quad \Delta \leftarrow 0 \\
\quad \text{for each } s \in \mathcal{S} \\
\quad\qquad \text{For each } a \in \mathcal{A}(s): \\
\quad\qquad\qquad q \leftarrow Q(s, a) \\
\quad\qquad\qquad Q(s, a) \leftarrow \sum_{s',r} p(s', r \mid s, a)
  \left[ r + \gamma \sum_{a'} \pi(a' \mid s')Q(s', a') \right] \\
\quad\quad\quad \Delta \leftarrow \max(\Delta, |q - Q(s,a)|) \\
\text{until } \Delta < \theta
\end{array}
$$,

for policy evaluation and 

$$
\hspace{0pt}
\begin{array}{l}
\text{policy-stable} \leftarrow \text{True} \\
\text{For each } s \in S: \\
\quad \text{old-action} \leftarrow \pi(s) \\
\quad \pi(s) \leftarrow \arg\max_a Q(s, a)\\
\quad \text{If old-action} \neq \pi(s) \text{, then policy-stable} \leftarrow \text{False} \\
\text{If policy-stable, stop and return:} \leftarrow Q \approx q_*, \pi \approx \pi_*,\\
\text{else, go to 2}
\end{array}
$$
 
for policy iteration. Main point here is that if you have the action-values equation, you don't really need to calculate it by expanding the Bellman equation: it's within the formulation of $$Q$$.

Something that almost tripped me up as I was typing out this solution was the realisation that you don't need to speculate on the action when working with $$Q$$-functions - you already *have* the action via the formulation! This manifests as the following change from the first formulation to the second:

$$
p(s', r \mid s, \pi(s)) \rightarrow p(s', r \mid s, a)
$$

## Remarks  
1. There is a weak correspondence between local attention and convolution kernels - I'm saying weak because I haven't yet looked too deeply into this. 
    * **Edit**: as it happens, the next paper that I need to read is called "Convolutional sequence to sequence learning". Nice. :)
2. Today was not a reinforcement learning day. Today was a `mathjax` day. It took me about two minutes to write the solution and an hour to get it rendering nicely!