---
layout: log
title: Statistical physics, my old friend
date: 2026-02-18
math: True
tags: [diffusion-models, reinforcement-learning]
---

## Summary
* Read *Deep unsupervised learning using non-equilibrium thermodynamics* by Sohl-Dickstein et al.
* Built a MDP generator.

## Notes
### Literature 
* The two basic paradigms of generative AI are diffusion models and GANs. From a brief search, it looks like GANs are faster and a little more efficient than diffusion models, whilst diffusion models are altogether more stable and produce higher qualities.
* **Basic idea**: diffusion models systematically and slowly destroy images by applying forward noise, after which a reverse diffusion process is employed to reconstitute the image.
* This works by transforming between low-entropy state to a high entropy state (specific image to noise dominated image). You can reconstitute an image from the high entropy state by following a diffusion processes guided by the input.
* The key here is the estimator - the reverse diffusion operator that we're trying to learn that predicts the perturbation so that we may reverse it.
* Property of diffusion that is being exploited here (the fact that they exist for any smooth target distribution) is really cool. This is the basic reason for their essential flexibility.
* "We show how to easily multiply the learned distribution with another probability distribution (eg with a conditional distribution in order to compute a posterior)" - hmm. Interesting. Could this be the way that a diffusion process is biased to produce particular images?
* **Forward diffusion**: a defined means of converting a complex data distribution into a simple and tractable distribution. 
* **Reverse diffusion**: a learned finite-time reversal of the forward diffusion process. This is the generative part of the methodology.
* A lot of the work here depends on the idea behind Kolmogorov diffusion operators, where a forward diffusion model and the backward diffusion model have the same functional form. This means that one can simply use MLPs to estimate the mean and covariance of the Gaussian kernel powering the reverse diffusion step.  
* Okay, so it's clear that exact calculation of the likelihood is intractable. The authors set a lower bound on the log-likelihood by applying Jensen's inequality, which in turn provides for a training objective represented by the KL-divergences that emerge after you set the forward distribution as a posterior. Oof - that is *smart*.
* The reverse diffusion kernel is then trained to minimise the above loss function. Okay, I get it!
* I need to implement this soon... goddamn, this stuff is **awesome**.

## Deep RL Experiments
1. Markov Decision process generator almost finished. I blasted this one out and it's basically done, but I haven't yet checked the output. 
2. I should mention that I'm not using AI for this. Is this wrong? Should I just use AI? I probably could have built a MDP generator in about two minutes if I did. Hmmm... 
3. No, I don't think I'll use AI for tasks that I don't know how to do like the back of my hand... which is pretty much most tasks!

## Remarks
* It's been about two years since I so much as touched statistical physics. This is a field that I spent years studying and trying to master. I'm not as rusty as I thought, but I'm still a little rusty. This is a bit of a shock to my system.
* The absolutely wild thing about this whole thing is that I **vividly** remember describing the use of diffusion as a potential learning method to one of my good friends and former fellow graduate students: he thought it was a stupid idea. :') 