---
layout: log
title: Transformers in the silicon
date: 2026-02-07
math: True
tags: [depth-first-search, deep-learning, reinforcement-learning]
---

## Summary
* Spent some time studying depth-first search techniques. 
* Implemented a transformer from scratch. 
* More progress on Sutton and Barto.

## Notes
1. Sat down and locked in to build this transformer. I discovered some fundamental misunderstandings on how multi-headed attention worked, but now I can honestly say that I understand what I'm talking about when I reference transformers. 
2. It took fucking *ages* to get the tensor gymnastics in multiheaded attention to work. I admit that for a few steps I did ask AI for suggestion on how to fix it, but in my defense the vaccines have still got me rinsed.
3. I actually forgot to implement masking in my transformer implementation. That... probably explains why the code wasn't running. I'll have this fixed by tomorrow, and this will be my architecture submission for the week. :)

## Sutton and Barto
1. *Maximisation bias* - an issue that seems to emerge whenever a greedy policy is used as an estimator. I'm guessing the issue here is when noise and uncertainty cause overestimates in the true value, which means that states with a value of zero are always taken by the greedy policy. What this basically means is that any action whose estimate is inflated by noise will be preferred over actions with more accurate (but lower-looking) estimates. 
2. The example is very illuminating, although I confess it took me a while to comprehend it. Essentially, the maximisation bias comes into play only when it takes time to figure out that a state has a negative expected rewards, especially if that negative reward is obscured by noise that demonstrates positive reward on occasion. 

## Remarks
* Started studying algorithms today. In fairness, I don't think that I'll be put into a situation where I'll have to pull out a binary tree inversion from scratch. I think the essence of what I'm trying to achieve here is to develop my ability to *think* in algorithmic systems. I can't call myself an algo engineer if I don't at least have a strong grasp on classic algorithms!
* As alluded to, I can now invert a binary tree. ~~In fairness, this is not that complicated at all: I wonder why the guy who built brew struggled with this?~~ **Edit**: It's toxic thinking like this that I'm trying very hard to get out of my system. **Of course** I don't find binary tree inversion difficult. I've been harping on about recursion being the coolest technique ever for about a decade now. Recursion is by no means the only way to solve this problem - it's worth respecting that people come from different backgrounds.
* This puts my current list of running missions at three:
    * Reinforcement learning,
    * Deep learning,
    * Algorithms
* I expect that algorithms will eventually evolve into robotics and control theory, which are fields that I am deeply fascinated in. However, I have to master these fundamentals first.  