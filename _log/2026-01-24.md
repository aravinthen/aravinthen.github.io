---
layout: log
title: Multi-headed inattention 
date: 2026-01-24
math: True
tags: [deep-learning]
---

## Summary
* Finished reading *Attention is All You Need* by Vaswani et al.  

## Notes
1. This is really... abstract. I might have to read this paper a couple of times.
2. The basic picture that I'm getting here is that transformers follow the encoder-decoder framework by
    * taking your inputs, embed them into a feature space and then give them a positional encoding,
    * calculating self-attention, which is a measure of the importance that each embedded token has to the other components of the sequence,
    * carry out a multi-layer attention process that distributes the context over many layers: this is somewhat similar to the way convolutional attention models worked. The output of this process is an embedding that is pretty much loaded with many levels of context,
    * feed forward these levels into the decoder to predict the most likely next token.
3. This is all well and good, but it's so multifaceted that I have no *idea* how all of this ties in together. The way self-attention actually works... what's going on here?
4. In fact, I don't think I have a mental model on how this works at all. I need more resources... in fact, I need to *build* a transformer to see what's really going on here. I think that'll be my task for tomorrow. 

## Remarks
* I spent the day with my cousin [one half of a brilliant filmmaking duo called MDFilms](https://www.mdfilms.uk), who seemingly came down with a cold halfway through our meeting. Turns out whatever he had is probably contagious, because I do not feel so good. :(   