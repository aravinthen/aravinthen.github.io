---
layout: log
title: Mind your P(olicie)s and Q(-value)s
date: 2026-02-06
math: True
tags: [graph-neural-networks, reinforcement-learning]
---

## Summary
* Read [*Deep Reinforcement Learning Doesn't Work Yet*](https://www.alexirpan.com/2018/02/14/rl-hard.html) by Alex Irpan.

## Notes
* It's definitely worth knowing about the shortcomings in RL as well as the successes. Granted, this paper was written about ten years ago: one can hope that some of the issues will have been dealt with by now.
* Here are the points in order:
    * **Deep reinforcement learning can be horrendously sample inefficient**: This is backed up with some compelling points in that the AI systems of time took 83 hours to learn an Atari game to near human performance. Essentially: *learning a policy will take more samples than you think it will.* I wonder however if this is something that could potentially be dealt with via the imposition of inductive bias, or other techniques that are intended to maximize the efficiency of training a network? It could be that modern AI systems, with the reams of inductive bias built in, can handle these issues. Granted, this is an important point to be aware of when training AI systems. 
    * **If You Just Care About Final Performance, Many Problems are Better Solved by Other Methods**: This.. is actually true, I think. I've seen GCN engineers at my company do some genuinely incredible stuff (which I will never share on this blog). Two notes, though: first, I think the place of DRL comes into play when there is simply too much complexity to even consider using a domain specific technique. Second, the claim at the end of this section where Boston Dynamics doesn't use RL is no longer accurate. 
    * **Reinforcement Learning Usually Requires a Reward Function**: yeah, this is a pain in the ass. The second point here is an elaboration of the first, but in general? I totally agree here. Reward shaping is genuinely very unfulfilling and definitely brings to light a ton of paradoxes within the methodology.
    * **Even Given a Good Reward, Local Optima Can Be Hard To Escape**: This is just another manifestation of the exploration-exploitation dilemma. It's true, as well - I really like the language used in the post to elaborate on this point. The way of thinking about behaviour as being *burned* into the policy at different stages is really useful. Further, following visualization is also a very pleasant and applicable mental model: *"I’ve taken to imagining deep RL as a demon that’s deliberately misinterpreting your reward and actively searching for the laziest possible local optima."*
    * **Even When Deep RL Works, It May Just Be Overfitting to Weird Patterns In the Environment**: Yup. Seen this before. Pain in the ass.
    * **Even Ignoring Generalization Issues, The Final Results Can be Unstable and Hard to Reproduce**: with the advent of more stable environments and well defined protocols, this is getting *somewhat* easier. It is still hard and required due-dilligence from day 1.
* Interestingly, a lot of the future directions described in the post were future directions that were taken. The results have been very promising so far, I think!
* I learned a new term today: *ablation study*. This is where you remove components of an AI system to quantify and assess the effect that the component had on the overall performance of the system.

### Sutton and Barto
* **Sarsa**. It's just a means of on-policy updating the $Q$ function. That is the main distinction here: with every step you take you update your prediction of $Q$. Functionally it's really not that complex!
* Worth reiterating that *Monte Carlo fails when policies don't terminate*. It is in this situation that you have to start thinking about using temporal difference methods in the first place.
* **Q-learning**. `Q-learning` updates your knowledge of the value by assuming that your agent is acting greedily. Where `sarsa` simply updates based on the next action take, `Q-learning` updates on the best possible action. It's off policy because you're *technically* not actually updating your knowledge based on it's actions. You're trying to approximate the optimal `Q-value` rather than the Q-value associated with the policy you're exploring.
* The example of why `Q-learning` might be more effective than `sarsa` is illuminating. Basically, when your target policy takes the highest value action consistently, it's easier to identify optimal behaviour... regardless of whether occasionally your behavioural policy walks you down a cliff.
 

## Remarks
* I'm still reeling from the vaccines that I had yesterday. Today was, alas, necessarily a quiet day. 