---
layout: log
title: Gradient ascent
date: 2026-02-23
math: True
tags: [reinforcement-learning]
---

## Summary
* Started reading Chapter 13 of Sutton and Barto in preparation for diving into policy gradient architectures. 

## Notes
1. Parameterized policy methods are techniques that dispense from the basic idea that you have to set actions based on your value function. The goal of a policy method is to learn a policy that selects optimal actions just by the way it's been parameterized. You can still use a value function to adjust the parameterization, but this class of techniques remove the interplay between the value function and the action.
2. The update of the policy is written as $$\theta_{t+1} = \theta_t + \alpha \mathbb{E}\left[\nabla J(\theta_t)\right]$$, which is reminiscient of gradient descent. The value of thinking about things in this formulation provides a really nice way of actually stating the problem: find a function $$J$$ that represents a scalar performance measure that can be optimized to improve the policy. Later in the chapter, we see that $$J$$ could be defined as being equal to the value function. 
3. **Policy-gradient** - any class of technique that improves a policy by the procedure described in point 2.
4. **Actor-critic** - a policy-gradient method that approximates an actor (the policy being learned) and a critic (a learned value function).
5. A key point: exploration in the policy-gradient methodology is fostered by the fact that a policy typically never becomes deterministic. **However**, given the gradient updates that continuously occur, it's possible that a policy gradient might eventually become deterministic. I feel like this could be a good thing or a bad thing depending on the sensitivity of the policy to the gradient update process...
6. Policy gradient approaches are also a handy way of handling continuous action spaces. This is fair: it's probably quite difficult to handle continuous action spaces with the methods described in the previous chapters. I suppose function approximation could be employed to handle this too...
7. **Soft-max in action preferences** - a natural way of building probability distributions for actors. 
8. The **essential key** here: policy gradient methods find the optimal policy. All the previous methods learn the value function, which is then used to learn the optimal policy.
9. Policy gradient approaches can demonstrate smoothness in their updates. This is not something that the $$\epsilon$$-greedy approaches typically permit. This usually allows for better convergence guarantees... although I wonder how applicable this is to the enormously noisy landscapes that are typically seen in modern DRL problems?
10. All the policy gradient theorem is: an *analytical expression* for the gradient of the performance measure with respect to the policy parameters. 
11. Oh, I see - I get what's going on with the `REINFORCE` algorithm!
    * The original derivation from the policy gradient theorem requires a sum over all actions.
    * In order to convert this to an expectation value (which in turn is something that you need to sample), you can't just pick all actions and hope it converges: some actions are taken frequently whereas some are not taken at all.
    * You can incorporate this for single actions by using a trick to *weight* actions. The mathematical details aside, the result of manipulating the original expression is to multiply and divide by the policy itself, which in turn becomes a logarithm of the policy gradient (good old logarithm tricks!).
12. `REINFORCE` is a Monte Carlo algorithm: you're still essentially sampling from the policy here.

## Deep RL project
1. Markov decision process generator fully developed and tested. Let's get some reinforcement algorithms in here (tomorrow)! :) 

## Remarks
* For work related reasons that will never see the light of day, I want to get a deeper understanding of policy gradient methods. As a result, this week's literature review will start by going through a book chapter.
* I mean, I already read blog posts as well as papers. Why not start reading relevant book chapters too?