---
layout: log
title: Gradient descent
date: 2026-02-24
math: True
tags: [reinforcement-learning]
---

## Summary
* Finished Chapter 13 of Sutton and Barto.

## Notes
1. Hmmm... this required a bit of time to digest, but roughly speaking:
    * `REINFORCE` with baseline serves as a means of reducing variance in the original reinforce method. Given that it employs $$V(s)$$, it is *not* a means of evaluating actions - the action doesn't even come into it: this method cannot tell you how good a particular action is.
    * Actor-critic methods, on the other hand, evaluate actions by incorporating the estimated value of the next state. You're basically using the temporal difference error as a learning signal over the value function.
    * I suppose that the same kind of distinction between temporal difference and Monte Carlo applies here: with `REINFORCE`, you have to fully evaluate your value function for it to be useful: your value function approximation must be a true estimate of the full process. Actor-critic, on the other hand, means that you just have to think about (and estimate) the difference between your current state and a potential future state.
    * Hold on, I forgot to think about why the baseline is useful in the first place: by using a baseline, you're reducing the noise that is inherent within the true return. It's almost like a normalizing feature, where you're steadily figuring out deviations over an average (in this case, the baseline).
2. No, I see now! Let's walk through this.
    * Your critic first evaluates the state of the actor's current position: it provides an *estimate* of what it thinks the value function will be at $$s$$.
    * The actor proposes an action, takes it, and then lands on a new position. It recieves an immediate reward in the process.
    * The critic then evaluates the state of the actor's new position and finds the difference between the old state-value estimate and the new.
    * You then tune your policy weights depending on how good the estimate was using gradient ascent. If the move was better than expected, increase the probability of that action.
    * You *also* tune your critic weights to provide a more accurate assessment of the value function. 
3. Whilst $$V$$ doesn't evaluate the action, the term $$G_t - V(s)$$ plays this role.
4. Now that I really think about it, the goal of the actor is to *surprise* the critic. The goal of the actor is to find actions that provide the maximum benefit. It does so by generating as much positive deviation from the critic estimate as possible.
5. All of this depends on the critic being a good estimator. If the critic provides poor value estimates, the actor will update in the wrong direction. Likewise, if you update the actor too rapidly, the value estimates that depend on what the actor samples become outdated. I suppose the instability will be compounded by bootstrapping, which is presumably why these methods are prone to collapsing in the first place. *Ah, hold up!* This is why proximal policy optimisation works!!! It stops the actor from moving too far and rendering the critic outdated!
6. The essence here is that the critic is *subjective*, whereas the baseline is not. 

## Remarks
* I came out of work at 6PM today, which left me little time to do any deep research. I'm currently working on shipping a big and complex feature and I really want to get this done. Sometimes, life gets in the way of research. 