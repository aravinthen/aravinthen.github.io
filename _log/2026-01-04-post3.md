---
layout: log
title: Shallow learning 
date: 2026-01-04
tags: [deep-learning]
---

## Summary
* Discovered a new platform called [PaperCode](https://papercode.vercel.app) that has a really nice "LeetCode" style to it. Spent a bit of time doing the ML150 exercises on the train back to Bristol. 
* Spent a quiet evening watching 3Blue1Brown's lecture series on [neural networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) as part of my research task for today. 

## Remarks
1. A rough pattern is emerging here. For the near future, I'm going to spend my weekdays studying reinforcement learning and my weekends focusing on the implementation of deep learning architectures. 
2. PaperCode is really good. I'm looking forward to blasting through it next week.
3. I'm not really a fan of video lectures. I find it too easy to get distracted, which is why I usually like to work through textbooks with pen and paper. However, 3Blue1Brown is a different matter entirely! I'm *very* familiar with neural networks in general, but seeing what I've already quite rigorously studied take form in the highly visual style of 3Blue1Brown is delightful. These are the like the Feynman Lectures of machine learning: a pleasure to come back to once you've become familiar with the topic. 
4. Seeing Grant's representation of the weights being nudged with every training example somewhat reminded me of how Grover's algorithm gently nudges a state vector to the correct quantum state via the Grover diffusion operator. [Turns out that some people have done work on this already](https://arxiv.org/html/2504.14568v1#:~:text=Figure%203%20shows%20the%20evolution,both%20training%20and%20test%20sets.).
5. It's funny how, once you've set yourself a research goal, an optimal plan emerges just by working on it rather than sitting around planning how you're going work on it. :) 