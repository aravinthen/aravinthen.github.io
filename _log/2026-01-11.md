---
layout: log
title: The Bellman equation
date: 2026-01-11
tags: [reinforcement-learning, software-engineering]
---

## Summary
* Progress on Sutton and Barto, covering Bellman equations. 
* Spent some time reading Efficient Python.

## Notes
1. It's quite fun to play around with the Bellman equation. It's so general that you can come up with thought experiments quite easily:
    * Come up with a game,
    * Generate the game tree of states and actions,
    * Associate all states with a reward,
    * Pick a particular state (the initial state),
    * Work backwards from the leaves of the game tree to find the full value function.
2. Of course, the it's hard to do any real calculations for anything remotely complicated, but it's cool to see the recursive value function "back-propagate" up the tree again.
3. The core of the Bellman equation is very simple: the value of any state is the expected reward that can be incurred in the next timestep in addition to the value of the next state. It's a really clever, subtle formulation of a difficult problem. 
4. Interestingly enough, I'm running into a bit of the exploration/exploitation dilemma myself right now. After having spent an hour sketching out general Markov games in my notebook, I'm wondering if I should have just carried on exploring with the book!